{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import textwrap\n",
    "import progressbar\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig, BertTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
    "from transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\n",
    "from transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig\n",
    "\n",
    "# MODEL_CLASSES = {\n",
    "#     'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n",
    "#     'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n",
    "#     'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n",
    "#     'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n",
    "#     'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)}\n",
    "\n",
    "# model_type = 'roberta' ###--> CHANGE WHAT MODEL YOU WANT HERE!!! <--###\n",
    "# model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]\n",
    "# model_name = 'roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Results/RoBERTa_Multi/pretrained/ were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"Results/RoBERTa_Multi/pretrained/\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = RobertaForSequenceClassification.from_pretrained(output_dir, output_hidden_states=True)\n",
    "model = RobertaForSequenceClassification.from_pretrained(output_dir)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(output_dir)\n",
    "model.to(device)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/ILDC_multi.csv')\n",
    "train_set = df.query(\" split=='train' \")\n",
    "test_set = df.query(\" split=='test' \")\n",
    "validation_set = df.query(\" split=='dev' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def att_masking(input_ids):\n",
    "  attention_masks = []\n",
    "  for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    attention_masks.append(att_mask)\n",
    "  return attention_masks\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_input_ids(all_toks):\n",
    "  splitted_toks = []\n",
    "  l=0\n",
    "  r=510\n",
    "  while(l<len(all_toks)):\n",
    "    splitted_toks.append(all_toks[l:min(r,len(all_toks))])\n",
    "    l+=410\n",
    "    r+=410\n",
    "\n",
    "  CLS = tokenizer.cls_token\n",
    "  SEP = tokenizer.sep_token\n",
    "  e_sents = []\n",
    "  for l_t in splitted_toks:\n",
    "    l_t = [CLS] + l_t + [SEP]\n",
    "    encoded_sent = tokenizer.convert_tokens_to_ids(l_t)\n",
    "    e_sents.append(encoded_sent)\n",
    "\n",
    "  e_sents = pad_sequences(e_sents, maxlen=512, value=0, dtype=\"long\", padding=\"post\")\n",
    "  att_masks = att_masking(e_sents)\n",
    "  return e_sents, att_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_for_one_vec(input_id, att_mask):\n",
    "  input_ids = torch.tensor(input_id)\n",
    "  att_masks = torch.tensor(att_mask)\n",
    "  input_ids = input_ids.unsqueeze(0)\n",
    "  att_masks = att_masks.unsqueeze(0)\n",
    "  model.eval()\n",
    "  input_ids = input_ids.to(device)\n",
    "  att_masks = att_masks.to(device)\n",
    "  with torch.no_grad():\n",
    "      # logits, encoded_layers = model(input_ids=input_ids, token_type_ids=None, attention_mask=att_masks)\n",
    "      logits = model(input_ids=input_ids, token_type_ids=None, attention_mask=att_masks)\n",
    "\n",
    "  # vec = encoded_layers[12][0][0]\n",
    "  # vec = vec.detach().cpu().numpy()\n",
    "  # return vec\n",
    "  # logits = logits.detach().cpu().numpy()\n",
    "  # label = np.argmax(logits, axis=1).flatten()\n",
    "  label = np.argmax(logits.logits.numpy(), axis=1).flatten()\n",
    "\n",
    "  return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_np_files_for_emb(dataf, tokenizer):\n",
    "  all_docs = []\n",
    "  ####\n",
    "  my_preds = []\n",
    "  ####\n",
    "  for i in range(len(dataf['text'])):\n",
    "    text = dataf['text'].iloc[i]\n",
    "    toks = tokenizer.tokenize(text, add_prefix_space=True)\n",
    "    if(len(toks) > 10000):\n",
    "      toks = toks[len(toks)-10000:]\n",
    "\n",
    "    splitted_input_ids, splitted_att_masks = grouped_input_ids(toks)\n",
    "\n",
    "    vecs = []\n",
    "    for index,ii in enumerate(splitted_input_ids):\n",
    "      vecs.append(get_output_for_one_vec(ii, splitted_att_masks[index]))\n",
    " \n",
    "    lval = 0.0\n",
    "    for i in vecs:\n",
    "      lval+=i\n",
    "\n",
    "    if(lval/len(vecs) > 0.5):\n",
    "      my_preds.append(1)\n",
    "    else:\n",
    "      my_preds.append(0)\n",
    "\n",
    "  return my_preds\n",
    "  #   one_doc = np.asarray(vecs)\n",
    "  #   all_docs.append(one_doc)\n",
    "\n",
    "  # all_docs = np.asarray(all_docs)\n",
    "  # return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994\n",
      "0.6348088531187123\n"
     ]
    }
   ],
   "source": [
    "preds_dev = generate_np_files_for_emb(validation_set, tokenizer)\n",
    "dev_labels = validation_set['label'].to_list()\n",
    "print(len(preds_dev))\n",
    "correct=0\n",
    "for index in range(len(preds_dev)):\n",
    "  if(preds_dev[index] == dev_labels[index]):\n",
    "    correct+=1\n",
    "\n",
    "print(correct/len(preds_dev))\n",
    "np.save(\"Results/RoBERTa_Multi/ensemble npy/RoBERTa_dev.npy\", preds_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import textwrap\n",
    "import progressbar\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def metrics_calculator(preds, test_labels):\n",
    "    cm = confusion_matrix(test_labels, preds)\n",
    "    TP = []\n",
    "    FP = []\n",
    "    FN = []\n",
    "    for i in range(0,2):\n",
    "        summ = 0\n",
    "        for j in range(0,2):\n",
    "            if(i!=j):\n",
    "                summ=summ+cm[i][j]\n",
    "\n",
    "        FN.append(summ)\n",
    "    for i in range(0,2):\n",
    "        summ = 0\n",
    "        for j in range(0,2):\n",
    "            if(i!=j):\n",
    "                summ=summ+cm[j][i]\n",
    "\n",
    "        FP.append(summ)\n",
    "    for i in range(0,2):\n",
    "        TP.append(cm[i][i])\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for i in range(0,2):\n",
    "        precision.append(TP[i]/(TP[i] + FP[i]))\n",
    "        recall.append(TP[i]/(TP[i] + FN[i]))\n",
    "\n",
    "    macro_precision = sum(precision)/2\n",
    "    macro_recall = sum(recall)/2\n",
    "    micro_precision = sum(TP)/(sum(TP) + sum(FP))\n",
    "    micro_recall = sum(TP)/(sum(TP) + sum(FN))\n",
    "    micro_f1 = (2*micro_precision*micro_recall)/(micro_precision + micro_recall)\n",
    "    macro_f1 = (2*macro_precision*macro_recall)/(macro_precision + macro_recall)\n",
    "    return macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6942946832843206,\n",
       " 0.6348088531187123,\n",
       " 0.663220583725073,\n",
       " 0.6348088531187123,\n",
       " 0.6348088531187123,\n",
       " 0.6348088531187123)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_calculator(preds_dev, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1517\n",
      "0.6242584047462096\n"
     ]
    }
   ],
   "source": [
    "preds_test = generate_np_files_for_emb(test_set, tokenizer)\n",
    "test_labels = test_set['label'].to_list()\n",
    "print(len(preds_test))\n",
    "correct=0\n",
    "for index in range(len(preds_test)):\n",
    "  if(preds_test[index] == test_labels[index]):\n",
    "    correct+=1\n",
    "\n",
    "print(correct/len(preds_test))\n",
    "np.save(\"Results/RoBERTa_Multi/ensemble npy/RoBERTa_test.npy\", preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6820188702068465,\n",
       " 0.6255462272513949,\n",
       " 0.6525630456204138,\n",
       " 0.6242584047462096,\n",
       " 0.6242584047462096,\n",
       " 0.6242584047462096)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_calculator(preds_test, test_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_for_one_vec(input_id, att_mask):\n",
    "  input_ids = torch.tensor(input_id)\n",
    "  att_masks = torch.tensor(att_mask)\n",
    "  input_ids = input_ids.unsqueeze(0)\n",
    "  att_masks = att_masks.unsqueeze(0)\n",
    "  model.eval()\n",
    "  input_ids = input_ids.to(device)\n",
    "  att_masks = att_masks.to(device)\n",
    "  with torch.no_grad():\n",
    "      # logits, encoded_layers = model(input_ids=input_ids, token_type_ids=None, attention_mask=att_masks)\n",
    "      logits = model(input_ids=input_ids, token_type_ids=None, attention_mask=att_masks)\n",
    "\n",
    "  # vec = encoded_layers[12][0][0]\n",
    "  # vec = vec.detach().cpu().numpy()\n",
    "  # return vec\n",
    "  # logits = logits.detach().cpu().numpy()\n",
    "  # label = np.argmax(logits, axis=1).flatten()\n",
    "  label = logits.logits[0][1].numpy().flatten()\n",
    "\n",
    "  return label\n",
    "\n",
    "def generate_np_files_for_emb(dataf, tokenizer):\n",
    "  all_docs = []\n",
    "  ####\n",
    "  my_preds = []\n",
    "  ####\n",
    "  for i in range(len(dataf['text'])):\n",
    "    text = dataf['text'].iloc[i]\n",
    "    toks = tokenizer.tokenize(text, add_prefix_space=True)\n",
    "    if(len(toks) > 10000):\n",
    "      toks = toks[len(toks)-10000:]\n",
    "\n",
    "    splitted_input_ids, splitted_att_masks = grouped_input_ids(toks)\n",
    "\n",
    "    vecs = []\n",
    "    for index,ii in enumerate(splitted_input_ids):\n",
    "      vecs.append(get_output_for_one_vec(ii, splitted_att_masks[index]))\n",
    " \n",
    "    lval = 0.0\n",
    "    for i in vecs:\n",
    "      lval+=i\n",
    "\n",
    "    if(lval/len(vecs) > 0.5):\n",
    "      my_preds.append(1)\n",
    "    else:\n",
    "      my_preds.append(0)\n",
    "\n",
    "  return my_preds\n",
    "  #   one_doc = np.asarray(vecs)\n",
    "  #   all_docs.append(one_doc)\n",
    "\n",
    "  # all_docs = np.asarray(all_docs)\n",
    "  # return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994\n",
      "0.5895372233400402\n"
     ]
    }
   ],
   "source": [
    "preds_dev = generate_np_files_for_emb(validation_set, tokenizer)\n",
    "dev_labels = validation_set['label'].to_list()\n",
    "print(len(preds_dev))\n",
    "correct=0\n",
    "for index in range(len(preds_dev)):\n",
    "  if(preds_dev[index] == dev_labels[index]):\n",
    "    correct+=1\n",
    "\n",
    "print(correct/len(preds_dev))\n",
    "np.save(\"Results/RoBERTa_Multi/ensemble npy/raw_logits_dev.npy\", preds_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.733028479912337,\n",
       " 0.5895372233400402,\n",
       " 0.6534988373189736,\n",
       " 0.5895372233400402,\n",
       " 0.5895372233400402,\n",
       " 0.5895372233400402)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_calculator(preds_dev, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1517\n",
      "0.5880026367831246\n"
     ]
    }
   ],
   "source": [
    "preds_test = generate_np_files_for_emb(test_set, tokenizer)\n",
    "test_labels = test_set['label'].to_list()\n",
    "print(len(preds_test))\n",
    "correct=0\n",
    "for index in range(len(preds_test)):\n",
    "  if(preds_test[index] == test_labels[index]):\n",
    "    correct+=1\n",
    "\n",
    "print(correct/len(preds_test))\n",
    "np.save(\"Results/RoBERTa_Multi/ensemble npy/raw_logits_test.npy\", preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7221584262317644,\n",
       " 0.5897855069440823,\n",
       " 0.6492938649870699,\n",
       " 0.5880026367831246,\n",
       " 0.5880026367831246,\n",
       " 0.5880026367831246)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_calculator(preds_test, test_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight later chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_for_one_vec(input_id, att_mask):\n",
    "  input_ids = torch.tensor(input_id)\n",
    "  att_masks = torch.tensor(att_mask)\n",
    "  input_ids = input_ids.unsqueeze(0)\n",
    "  att_masks = att_masks.unsqueeze(0)\n",
    "  model.eval()\n",
    "  input_ids = input_ids.to(device)\n",
    "  att_masks = att_masks.to(device)\n",
    "  with torch.no_grad():\n",
    "      # logits, encoded_layers = model(input_ids=input_ids, token_type_ids=None, attention_mask=att_masks)\n",
    "      logits = model(input_ids=input_ids, token_type_ids=None, attention_mask=att_masks)\n",
    "\n",
    "  # vec = encoded_layers[12][0][0]\n",
    "  # vec = vec.detach().cpu().numpy()\n",
    "  # return vec\n",
    "  # logits = logits.detach().cpu().numpy()\n",
    "  # label = np.argmax(logits, axis=1).flatten()\n",
    "  label = np.argmax(logits.logits.numpy(), axis=1).flatten()\n",
    "\n",
    "  return label\n",
    "\n",
    "def generate_np_files_for_emb(dataf, tokenizer):\n",
    "  all_docs = []\n",
    "  ####\n",
    "  my_preds = []\n",
    "  ####\n",
    "  for i in range(len(dataf['text'])):\n",
    "    text = dataf['text'].iloc[i]\n",
    "    toks = tokenizer.tokenize(text, add_prefix_space=True)\n",
    "    if(len(toks) > 10000):\n",
    "      toks = toks[len(toks)-10000:]\n",
    "\n",
    "    splitted_input_ids, splitted_att_masks = grouped_input_ids(toks)\n",
    "\n",
    "    vecs = []\n",
    "    for index,ii in enumerate(splitted_input_ids):\n",
    "      vecs.append(get_output_for_one_vec(ii, splitted_att_masks[index]))\n",
    " \n",
    "    lval = 0.0\n",
    "    for weight, i in enumerate(vecs):\n",
    "      lval+=(weight+1)*i\n",
    "\n",
    "    if(lval/sum(range(len(vecs)+1)) > 0.5):\n",
    "      my_preds.append(1)\n",
    "    else:\n",
    "      my_preds.append(0)\n",
    "\n",
    "  return my_preds\n",
    "  #   one_doc = np.asarray(vecs)\n",
    "  #   all_docs.append(one_doc)\n",
    "\n",
    "  # all_docs = np.asarray(all_docs)\n",
    "  # return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994\n",
      "0.6569416498993964\n"
     ]
    }
   ],
   "source": [
    "preds_dev = generate_np_files_for_emb(validation_set, tokenizer)\n",
    "dev_labels = validation_set['label'].to_list()\n",
    "print(len(preds_dev))\n",
    "correct=0\n",
    "for index in range(len(preds_dev)):\n",
    "  if(preds_dev[index] == dev_labels[index]):\n",
    "    correct+=1\n",
    "\n",
    "print(correct/len(preds_dev))\n",
    "np.save(\"Results/RoBERTa_Multi/ensemble npy/weighted_natural_dev.npy\", preds_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7322429906542056,\n",
       " 0.6569416498993963,\n",
       " 0.6925514497712028,\n",
       " 0.6569416498993964,\n",
       " 0.6569416498993964,\n",
       " 0.6569416498993964)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_calculator(preds_dev, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1517\n",
      "0.6413974950560316\n"
     ]
    }
   ],
   "source": [
    "preds_test = generate_np_files_for_emb(test_set, tokenizer)\n",
    "test_labels = test_set['label'].to_list()\n",
    "print(len(preds_test))\n",
    "correct=0\n",
    "for index in range(len(preds_test)):\n",
    "  if(preds_test[index] == test_labels[index]):\n",
    "    correct+=1\n",
    "\n",
    "print(correct/len(preds_test))\n",
    "np.save(\"Results/RoBERTa_Multi/ensemble npy/weighted_natural_test.npy\", preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7077636780538669,\n",
       " 0.6426917661782343,\n",
       " 0.6736599718680749,\n",
       " 0.6413974950560316,\n",
       " 0.6413974950560316,\n",
       " 0.6413974950560316)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_calculator(preds_test, test_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base transformer model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_id_maker(dataf, tokenizer):\n",
    "  input_ids = []\n",
    "  lengths = []\n",
    "\n",
    "  for i in range(len(dataf['text'])):\n",
    "    sen = dataf['text'].iloc[i]\n",
    "    sen = tokenizer.tokenize(sen, add_prefix_space=True)\n",
    "    CLS = tokenizer.cls_token\n",
    "    SEP = tokenizer.sep_token\n",
    "    if(len(sen) > 510):\n",
    "      sen = sen[len(sen)-510:]\n",
    "\n",
    "    sen = [CLS] + sen + [SEP]\n",
    "    encoded_sent = tokenizer.convert_tokens_to_ids(sen)\n",
    "    input_ids.append(encoded_sent)\n",
    "    lengths.append(len(encoded_sent))\n",
    "\n",
    "  input_ids = pad_sequences(input_ids, maxlen=512, value=0, dtype=\"long\", truncating=\"pre\", padding=\"post\")\n",
    "  return input_ids, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_input_ids, validation_lengths = input_id_maker(validation_set, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def att_masking(input_ids):\n",
    "  attention_masks = []\n",
    "  for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    attention_masks.append(att_mask)\n",
    "  return attention_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_attention_masks = att_masking(validation_input_ids)\n",
    "validation_labels = validation_set['label'].to_numpy().astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_inputs = validation_input_ids\n",
    "validation_masks = validation_attention_masks\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max batch size should be 6 due to colab limits\n",
    "batch_size = 6\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = RandomSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = test_set.label.to_numpy().astype(int)\n",
    "\n",
    "input_ids, input_lengths = input_id_maker(test_set, tokenizer)\n",
    "attention_masks = att_masking(input_ids)\n",
    "\n",
    "# Convert to tensors.\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 6  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 1,517 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "model.eval()\n",
    "\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "for (step, batch) in enumerate(prediction_dataloader):\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7125906394199077"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "labels_flat = true_labels.flatten()\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "flat_accuracy(predictions,true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import textwrap\n",
    "import progressbar\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import time\n",
    "import datetime\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7225405297460197 0.7130712137803967 0.7177746418902077 0.7125906394199077 0.7125906394199077 0.7125906394199077\n"
     ]
    }
   ],
   "source": [
    "def metrics_calculator(preds, test_labels):\n",
    "    cm = confusion_matrix(test_labels, preds)\n",
    "    TP = []\n",
    "    FP = []\n",
    "    FN = []\n",
    "    for i in range(0,2):\n",
    "        summ = 0\n",
    "        for j in range(0,2):\n",
    "            if(i!=j):\n",
    "                summ=summ+cm[i][j]\n",
    "\n",
    "        FN.append(summ)\n",
    "    for i in range(0,2):\n",
    "        summ = 0\n",
    "        for j in range(0,2):\n",
    "            if(i!=j):\n",
    "                summ=summ+cm[j][i]\n",
    "\n",
    "        FP.append(summ)\n",
    "    for i in range(0,2):\n",
    "        TP.append(cm[i][i])\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for i in range(0,2):\n",
    "        precision.append(TP[i]/(TP[i] + FP[i]))\n",
    "        recall.append(TP[i]/(TP[i] + FN[i]))\n",
    "\n",
    "    macro_precision = sum(precision)/2\n",
    "    macro_recall = sum(recall)/2\n",
    "    micro_precision = sum(TP)/(sum(TP) + sum(FP))\n",
    "    micro_recall = sum(TP)/(sum(TP) + sum(FN))\n",
    "    micro_f1 = (2*micro_precision*micro_recall)/(micro_precision + micro_recall)\n",
    "    macro_f1 = (2*macro_precision*macro_recall)/(macro_precision + macro_recall)\n",
    "    return macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1\n",
    "\n",
    "macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(pred_flat, labels_flat)\n",
    "print(macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "prediction_sampler = SequentialSampler(validation_data)\n",
    "prediction_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 994 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "print('Predicting labels for {:,} test sentences...'.format(len(validation_inputs)))\n",
    "model.eval()\n",
    "\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "for (step, batch) in enumerate(prediction_dataloader):\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "  with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7193158953722334"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "pred_flat = np.argmax(predictions, axis=1).flatten()\n",
    "labels_flat = true_labels.flatten()\n",
    "\n",
    "flat_accuracy(predictions,true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7416193891386569 0.7193158953722334 0.7302973931556175 0.7193158953722334 0.7193158953722334 0.7193158953722335\n"
     ]
    }
   ],
   "source": [
    "macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1 = metrics_calculator(pred_flat, labels_flat)\n",
    "print(macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1b10f52dcfbd873cf86907d258d45f4ca617b07ff582c487991c2d8bff2bfb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
